[[!meta title="Automated ISO/IMG tests on Jenkins"]]

[[!toc levels=1]]

# For developers

## Full test suite vs. scenarios tagged `@fragile`

Jenkins generally only runs scenarios that are _not_ tagged `@fragile`
in Gherkin. But it runs the full test suite, including scenarios that
are tagged `@fragile`, if the images under test were built:

 - from a branch whose name ends with the `+force-all-tests` suffix
 - from a tag
 - from the `devel` branch
 - from the `testing` branch
 - from the `feature/tor-nightly-master` branch

Therefore, to ask Jenkins to run the full test suite on your topic
branch, give it a name that ends `+force-all-tests`.

## Trigger a test suite run without rebuilding images

Every `build_Tails_ISO_*` job run triggers a test suite run
(`test_Tails_ISO_*`), so most of the time, we don't need
to manually trigger test suite runs.

However, in some cases, all we want is to run the test suite multiple
times on a given set of Tails images, that were already built. In such
cases, it is useless and problematic to trigger a build job, merely to
get the test suite running eventually:

 - It's a waste of resources: it will keep isobuilders uselessly busy,
   which makes the feedback loop longer for our other team-mates.
 - It forces us to wait at least one extra hour before we get the
   test suite feedback we want.

Thankfully, there is a way to trigger a test suite run without having
to rebuild images first. To do so, start a "build" of the
corresponding `test_Tail_ISO_*` job, passing to the
`UPSTREAMJOB_BUILD_NUMBER` parameter the ID of the `build_Tail_ISO_*`
job build you want to test.

<div class="caution">
Do <strong>not</strong> directly start a <code>test_Tail_ISO_*</code> job:
this is not supported. It would fail most of the time in confusing ways.
</div>

# For sysadmins

## Old ISO used in the test suite in Jenkins

Some tests like upgrading Tails are done against a Tails installation made from
the previously released ISO.

In some cases (e.g when the _Tails Installer_ interface has changed), we need to
temporarily change this behaviour to make tests work. To have Jenkins
use the ISO being tested instead of the last released one:

1. Set `USE_LAST_RELEASE_AS_OLD_ISO=no` in the
   `macros/test_Tails_ISO.yaml` and
   `macros/manual_test_Tails_ISO.yaml` files in the
   `jenkins-jobs` Git repository
   (`gitolite@git.puppet.tails.boum.org:jenkins-jobs`).

   Documentation and policy to access this repository is the same as
   for our [[Puppet modules|contribute/git#puppet]].

   See for example
   [commit 371be73](https://git-tails.immerda.ch/jenkins-jobs/commit/?id=371be73).

   <div class="note">
   Treat the repository at immerda as a read-only mirror: any change
   pushed there does not affect our infrastructure and will
   be overwritten.
   </div>

   Under the hood, once this change is applied Jenkins will pass the
   ISO being tested (instead of the last released one) to
   `run_test_suite`'s `--old-iso` argument.

2. File a ticket to ensure this temporarily change gets reverted
   in due time.

## Retrieving the ISOs for the test

We'll need a way to retrieve the different ISO needed for the test.

For the ISO related to the upstream build job, this shouln't be a
problem with #9597. We can get it with either wget, or a python script
using python-jenkins. That was the point of this ticket.

For the last release ISO, we have several means:

* Using wget to get them from http://iso-history.tails.boum.org. This
  website is password protected, but we could set up another private
  vhost for the isotesters.
* Using the git-annex repo directly.

We'll use the first one, as it's easier to implement.

## Restarting slave VMs between jobs

This question is tracked in [[!tails_ticket 9486]].

For [[!tails_ticket 5288]] to be robust enough, if the test suite doesn't
_always_ clean between itself properly (e.g. when tests simply hang
and timeout), we might want to restart `isotesterN.lizard` between
each each ISO testing job.

If such VMs are Jenkins slave, then we can't do it as part of the job
itself, but workarounds are possible, such as having a job restart and
wait for the VM, that triggers another job that actually starts the
tests. Or, instead of running `jenkins-slave` on those VMs, running
one instance thereof somewhere else (in a Docker container on
`jenkins.lizard`?) and then have "restart the testing VM and wait for
it to come up" be part of the testing job.

This was discussed at least there:

* <http://jenkins-ci.361315.n4.nabble.com/How-to-reboot-a-slave-during-a-build-td4628820.html>
* <https://stackoverflow.com/questions/5543413/reconfigure-and-reboot-a-hudson-jenkins-slave-as-part-of-a-build>

We achieve this VM reboot by using 3 chained jobs:

* First one is a wrapper and trigger 2 other jobs. It is executed on the
  isotester the test job is supposed to be assigned to. It puts the
  isotester in offline mode and starts the second job, blocking while
  waiting for it to complete. This way this isotester is left reserved
  while the second job run, and the isotester name can be passed as a build
  parameter to the second job. This job is low prio so it waits for
  other second and third type of jobs to be completed before starting its
  own.
* The second job is executed on the master (which has 4 build
  executors). This job ssh into the said isotester and issue the
  reboot. It needs to wait a reasonable amount of time for the Jenkins
  slave to be stopped by the shutdown process so that no jobs gets assigned
  to this isotester meanwhile. Stoping this Jenkins slave daemon usually
  takes a few seconds. During testing, 5 seconds proved to be enough of
  a delay for that, and more would mean unnecessary lagging time. It then
  put the node back online again.  This job is higher prio so that it is
  not lagging behind other wrapper jobs in the queue.
* The third job is the test job, run on the freshly started isotester.
  This one is high prio too to get executed before any other wrapper
  jobs. These jobs are set to run concurrently, so that if a first one is
  already running, a more recent one triggered by a new build will still
  be able to run and not be blocked by the first running one.
