[[!meta title="Tails February 2016 report"]]

[[!toc levels=2]]

<div class="caution">
<strong>Deadline: 2016-03-05</strong>
</div>

<div class="note">
Deliverable identifiers and descriptions are not free-form: they must
be copy'n'pasted as-is from the proposal sent to the sponsor.
</div>

[Last month's activity on Redmine](https://labs.riseup.net/code/projects/tails/issues?query_id=208)
can be helpful.

This reports covers the activity of Tails in February 2016.

Everything in this report can be made public.

# Z. Example section title

## Z.n. description of subsection

- A.n.m. description of deliverable: ticket numbers

  status summary:

  * what was done
  * what is the outcome (how it makes Tails better)
  * what was not done, and why

# A. Replace Claws Mail with Icedove

XXX: u

# B. Improve our quality assurance process

## B.1. Automatically build ISO images for all the branches of our source code that are under active development

XXX: bertagaz

## B.2. Continuously run our entire test suite on all those ISO images once they are built

XXX: bertagaz

## B.3. Extend the coverage of our test suite

XXX: anonym

## B.4. Freezable APT repository

This project was still on hold in February, while the developer
responsible for this project was focusing on other matters; we will
resume work on it in March. However, we mistakenly scheduled for
milestone V (April 15) two big projects that have the same developers,
so we decided to spread them more evenly over the remaining five
months of this contract; in April we will focus on C.1 (Change in
depth the infrastructure of our pool of mirrors), and here is the
updated schedule for the freezable APT repository project.

By the end of March, we want to:

* complete the design and discussion phase, that is "B.4.1.
  Specify when we want to import foreign packages into which APT
  suites" ([[!tails_ticket 9488]]), and "B.4.4. Design freezable APT
  repository" ([[!tails_ticket 9487]]);

* make enough progress on "B.4.2. Implement a mechanism to save the
  list of packages used at ISO build time" so it can be merged in
  April, ideally in time for Tails 2.3;

* have a working proof-of-concept for most other essential pieces of
  infrastructure and code.

Then, after a hiatus in April while we will be focused on our pool of
HTTP mirrors, in May we want to improve the freezable APT repository
as needed, aiming at merging code into the main development branch,
and deploying all pieces of infrastructure in production, by the end
of the month. Our current goal is to build Tails 2.4 (scheduled on
June 7th) using our freezable APT repository.

And then, we will still have two months, until the end of the
contract. This slack might be needed if previous steps take more time
than expected, and if not it will be time for us to identify remaining
issues, gather feedback from release managers and developers, and to
improve tools and documentation as we deem necessary.

# C. Scale our infrastructure

## C.1. Change in depth the infrastructure of our pool of mirrors

* C.1.1. Specify a way of describing the pool of mirrors
  ([[!tails_ticket 8637]])

  We've designed a file format, encoded it into a JSON schema, created
  a simple validation script, and published an example configuration
  file.

  We have discussed with the developers of the Download And
  Verification Extension (DAVE) for Firefox how it will be able to
  leverage this configuration file, and the code we are writing for
  "C.1.2. Write & audit the code that makes the redirection decision
  from our website", so that DAVE uses our new mirror pool design
  ([[!tails_ticket 10284]]). This discussion made us confident that
  what we have been working on so far is compatible with DAVE.

* C.1.3. Design and implement the mirrors pool administration process
  and tools ([[!tails_ticket 8638]], [[!tails_ticket 11122]])

  Building on top of what was done for C.1.1, a way to convey the
  mirror pool's configuration to the dispatcher script, based on
  ikiwiki underlays and Git, was designed and implemented.

Finally, we have organized our team to work on the next steps of this
project. A dedicated sprint will take place in April, during which we
want to complete all the needed programming, documentation and setup
tasks. Actual deployment might require more time, though: depending on
how fast mirror operators are to adjust to the new setup, we may have
to postpone the production deployment to May.

## C.2. Be able to detect within hours failures and malfunction on our services

- C.2.1. Research and decide what monitoring solution to use
         what tools and abstraction layer to use for configuring it,
         and where to host it: [[!tails_ticket 8645]]

  We settled on a plan while refining the details of the implementation
  of Icinga2 in our infrastructure.

  We agreed to use its decentralized feature to isolate our monitored
  systems from our monitoring one: a VM on the monitored host will be
  set up as a Icinga2 `satellite`, and will collect the datas from the
  other monitored systems, to send them back to the monitoring system
  Icinga2 instance. The later will be the only one responsible for the
  sending of notifications and will also be the one running the network
  checks.

  Icinga2 will be the agent we'll use on all systems to collect
  monitoring datas.

  We've also settled on the way to secure the communication between our
  systems, and decided not to solely rely on icinga2 SSL certificates,
  but to harden it using a VPN.

  This was also necessary, because we chose to manage our monitoring
  system with our current puppetmaster, which is hosted on the monitored
  host. So both Icinga2 and puppet take benefits from this VPN.
  [[!tails_ticket 10760]]

  Still some of the deep details are quite blurry for the reviewer of
  this design. We'll leave this discussion open, so that we can go on
  with the deployement, while we'll be able discuss some other questions
  that may raise later.

- C.2.2. Set up the monitoring software and the underlying infrastructure

  We've deployed the VPN between our systems [[!tails_ticket 11094]],
  which lead us to finish the install of the OS on the monitoring
  machine. It's now managed by our puppetmaster as any other of our
  systems. [[!tails_ticket 8647]]

  We also installed the VM on our monitored host that will serve as the
  satellite relay to our monitoring system. [[!tails_ticket 10886]]

  We then started writing in our puppet manifests the recipes we learned
  from the monitoring prototype tested on a developer machine. We had
  Icinga2 installed on all of our systems with a basic configuration.
  Then we configured it on the monitoring system as well as on the VM that
  will be the satellite so that they are now both interconnected over
  the VPN. [[!tails_ticket 8648]]

  We still need to connect our Icinga2 agent instances on the rest of
  our systems to this Icinga2 network. This will be done in the beginning
  of March, and we'll then be able to implement the various checks we
  defined in the blueprint, which are part of C.2.4 and C.2.6. Once
  done, in end of March we'll configure the notifications (C.2.5) and
  will release our monitoring setup for the end of M5.

## C.4. Maintain our already existing services

XXX: intrigeri

* We made plans to upgrade to Debian 8 (Jessie) the small number of
  Debian 7 (Wheezy) systems we still have ([[!tails_ticket 11178]],
  [[!tails_ticket 11186]]).

# D. Migration to Debian Jessie

As reported last month, all remaining deliverables were completed
in January.

Still, as a follow-up we upgraded our ISO build system to Debian
Jessie, and then updated our Vagrant basebox and Jenkins ISO builders
accordingly ([[!tails_ticket 9262]]).

# E. Release management

XXX: anonym
