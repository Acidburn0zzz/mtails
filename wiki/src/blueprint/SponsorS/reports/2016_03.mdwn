[[!meta title="Tails March 2016 report"]]

[[!toc levels=2]]

<div class="caution">
<strong>Deadline: 2016-04-08</strong>
</div>

<div class="note">
Deliverable identifiers and descriptions are not free-form: they must
be copy'n'pasted as-is from the proposal sent to the sponsor.
</div>

[Last month's activity on Redmine](https://labs.riseup.net/code/projects/tails/issues?query_id=208)
can be helpful.

This reports covers the activity of Tails in March 2016.

Everything in this report can be made public.

# A. Replace Claws Mail with Icedove

- A.1.1 Secure the Icedove autoconfig wizard: #6154

Now that we've adapted Thunderbird to use the automatic email creation wizard again, we also needed to adjust some settings in Torbirdy, in order to benefit from both. We've made a lot of progress on adapting Torbirdy's code (#11204) and have sent the [modifications upstream](https://github.com/ioerror/torbirdy/pull/30).

We have done the same last month with the modifications we'd like to see in Thunderbird's code. However, all of our patches are still waiting for review.

In Tails itself, we have set up a development branch which includes all those modifications, uses the proxy and uses only secure protocols while still allowing the user to benefit from the automatic email creation wizard.
However, we're hesitating to release because we want the Icedove and Torbirdy parts to be upstreamed first, so we don't risk releasing something that would change on the user's end if upstream prefers a different implementation than ours (e.g. different GUI). For example, Thunderbird upstream has not yet agreed to our proposal of implementing a checkbox, which allows the user to un/check ""Only use secure protocols" and we are waiting for a discussion which would allow for implementing a strategy which we equally agree to.

Our ideal timeline for releasing these modifications to Tails aims at the Tails 2.4 release. But as said, we'd like to have more comments from Thunderbird beforehand.

# B. Improve our quality assurance process

## B.1. Automatically build ISO images for all the branches of our source code that are under active development

In March, **839 ISO images** were automatically built by our Jenkins
instance.

## B.2. Continuously run our entire test suite on all those ISO images once they are built

In March, **834 ISO images** were automatically tested by our Jenkins
instance.

- B.2.4. Implement and deploy the best solution from this research: [[!tails_ticket 5288]]

  Given the workload on other deliverables, we had to postpone the
  February triaging of false positives in our test infrastructure. It
  will be handled at the same time as the March one. ([[!tails_ticket
  11083]] and [[!tails_ticket 11084]])

  We've rejected a bug in the Cucumber Jenkins plugin that didn't
  reappear recently. It was likely due to a manipulation we did at that
  time. ([[!tails_ticket 10725]])

  In the same vein, another bug is likely to be closed, as we didn't see
  it again. ([[!tails_ticket 10601]])

## B.3. Extend the coverage of our test suite

### B.3.11. Fix newly identified issues to make our test suite more robust and faster

- Robustness improvements

  As discussed in this section in the previous report, this month has
  been spent on trying out new "fundamental approaches" to deal with
  our robustness issues. Specifically we have looked at dealing with
  "glitches when interacting with graphical user interfaces", and we
  found a suitable solution in
  [Dogtail](https://fedorahosted.org/dogtail/), an automated GUI
  testing framework based on assistive technologies (specifically
  a11y). ([[!tails_ticket 10721]])

  We have developed a simple interface in our test suite to generate
  and run Dogtail scripts inside the system under test, so elements of
  the graphical user interface can be identified *programmatically*
  and then interacted with much like before (e.g. clicked with the
  mouse, or typed text into). Previously we could only identify such
  elements using static images that had to exactly match what we see
  on the screen, which often turned out to not be the case due to the
  non-deterministic nature of modern desktop applications, and this
  new approach already shows promise on being more reliable.

  Furthermore, being able to identify elements with code instead of
  images will significantly decrease the maintenance burden;
  previously we've had to update images whenever something changes in
  the GUI, e.g. size or anti-aliazing changes for fonts, whereas we
  now only need to change if the program itself changes. Also, when
  reading the code, referring to an image is much more opaque than a
  programmatic description, making the code more readable.

  Next month's focus will be to deal with "transient network
  issues" ([[!tails_ticket 9521]]).

- Performance improvements on Jenkins

  Last month, we started optimizing the platform that runs our test
  suite to make it run faster. As planned, in March we came back to
  it, and measurements showed that real workloads indeed benefit from
  these changes ([[!tails_ticket 11175]], [[!tails_ticket 11113]]).

## B.4. Freezable APT repository

In summary, we now have a working proof-of-concept for all essential pieces of
infrastructure and code, except some bits of the "tagged snapshots" component
(that is kind of bonus material for the time being, and is not needed to achieve
the goals we have set about this deliverable this year).

The progress we made in March matches the updated development schedule that we
shared in [[last month's report|contribute/reports/SponsorS/2015/2016_02]], and
the goals we have set back then still seem realistic.

Now, into the details:

- B.4.1. Specify when we want to import foreign packages into which APT suites ([[!tails_ticket 9488]])
  and B.4.4. Design freezable APT repository ([[!tails_ticket 9487]])

  We [discussed](https://mailman.boum.org/pipermail/tails-dev/2016-March/010369.html)
  our current proposed design and validated it. Proof-of-concept implementations
  also confirm the soundness of large parts of this design, so we now consider
  these two deliverables as done.

- B.4.2. Implement a mechanism to save the list of packages used at ISO build time ([[!tails_ticket 10748]])

  We made great progress on this front. We added support for storing the
  architecture information about packages used at build time, in preparation for
  (unrelated) upcoming changes that will make the Tails build process fetch
  packages for multiple architectures. We made our code and design evolve a bit
  after more intensive testing that highlighted a few problems.

  Some polishing remains to be done, but we are happy with how this component
  now works. We still aim at having this work merged in April, ideally in time
  for Tails 2.3 (which will require an exception since this release is
  a bugfix-only one).

  This will allow us to store (virtually forever) all packages needed to build
  a given Tails release.

XXX:intrigeri:

- B.4.3. Centralize and merge the list of needed packages

  This deliverable doesn't make sense, as-is, in our current design. It is
  replaced by processes and tools that our release managers can use to:

  * keep around the APT snapshots we need for a longer time than the default,
    e.g. the ones used during a longer than usual Tails code freeze, and prevent
    them from being deleted by our garbage collector;

  * freeze and thaw time-based APT snapshots used by a branch.

  The corresponding programs were drafted
  (<https://git-tails.immerda.ch/tails/tree/auto/scripts/apt-snapshots-serials?h=feature/build-from-snapshots>,
  <https://git-tails.immerda.ch/puppet-tails/tree/files/reprepro/snapshots/time_based/tails-bump-apt-snapshot-valid-until>)
  and passed early testing. We will now document their usage as part of the
  Tails release process documentation.

- B.4.5. Implement processes and tools for importing and freezing those packages

  Time-based snapshots are now generated four times a day, and
  [published over HTTP](http://time-based.snapshots.deb.tails.boum.org/).
  We designed how to do garbage collecting of these snapshots, and have a draft
  implementation
  (<https://git-tails.immerda.ch/puppet-tails/tree/files/reprepro/snapshots/time_based/tails-delete-expired-apt-snapshots>).
  This infrastructure is described with, and managed by, Puppet:
  <https://git-tails.immerda.ch/puppet-tails/tree/manifests/reprepro>

  We have successfully built a Tails ISO image using time-based APT snapshots:
  [[!tails_gitweb_branch feature/build-from-snapshots]]! This was a great
  success for us, as it allowed us to validate a large chunk of our previous
  work in a production context.

  This lead us to identify that if we didn't do anything about it, the
  `apt-cacher-ng` caching proxy used for building Tails ISO images, that
  improves build speed a lot, would not only be rendered inefficient, but would
  also consume quickly growing amount of disk space. It took us some time but we
  now have a proof-of-concept solution for that problem. Next step on this front
  is to .

- B.4.6. Adjust the rest of our ecosystem to the freezable APT repository ([[!tails_ticket 6303]])

  We investigated more closely what exact changes remain to be done, adjusted
  our backup system, and ensured that our plans for a fail-over system already
  take into account the needs of our upcoming freezable APT repository.

  What remains to be done (purchasing a hard-drive for an additional set of
  backups) is minor and not blocking, so we can consider this deliverable as
  basically done at this point.

# C. Scale our infrastructure

## C.1. Change in depth the infrastructure of our pool of mirrors

## C.2. Be able to detect within hours failures and malfunction on our services

- C.2.2. Set up the monitoring software and the underlying infrastructure

  During the last round, we deployed a basic Icinga2 instance on every
  systems that needed to be monitored. They weren't configured to check
  anything nor communicate with anyone. We just connected the virtual
  machine acting as a Icinga2 *satellite* to the monitoring host over
  a VPN.

  During this month, we configured half of our monitored systems as
  Icinga2 agents reporting to the satellites.

  This required us to write a second batch of Puppet manifests,
  so deploying the other half of our monitored agents later will help to
  test if we introduce any *from scratch* deployment bug during the rest
  of the development process. Integrating them won't require much
  efforts to do as everything is automated.

  The satellite VM is collecting the reports and pushing them to our
  monitoring system. This way, our infrastructure hosted on one host
  is isolated from the external monitoring host, which can't execute
  any code on the monitoring systems and only collect results pushed to
  it.

  We preferred to use Puppet to spread the configuration, so that we
  don't have to use Icinga2 own ways to do it, isolating a bit more
  both hosts.

  We also set up a Icingaweb2 interface at
  https://icingaweb2.tails.boum.org, so that we could have feedbacks
  from the systems.

- C.2.4. (Configure and debug the monitoring of the most critical
  services) and C.2.6. (Configure and debug the monitoring of other
  high-priority services)

  We then deployed basic system checks that were part of our previously
  defined objectives, as well as set up our monitoring host to check
  public services from a remote point of view. Deploying this checks
  already warned us some of our partitions were closed to be full.

  Most of our checks defined as `CRITICAL` and `HIGH PRIORITY` are now
  deployed. Two of them are being implemented as custom Icinga2 plugins
  as they are not common. ([[!tails_ticket 8650]] and [[!tails_ticket 8653]])

- C.2.5. Configure the receiving side of the monitoring notifications

  As planned we're going to spend the next round to set up the
  notification process, last mile to the release of out monitoring
  system.

## C.4. Maintain our already existing services

We kept on answering the requests from the community as well as taking
care of security updates as covered by "C.4.5. Administer our services
up to milestone V".

* We upgraded to Debian 8 (Jessie) all the Debian 7 (Wheezy) systems
  we were still running ([[!tails_ticket 11178]],
  [[!tails_ticket 11186]]). This fixed a bug that prevented us to
  upgrade the Linux kernel running on all our ISO tester virtual
  machines ([[!tails_ticket 9157]]).

XXX: intrigeri

# D. Migration to Debian Jessie


# E. Release management
